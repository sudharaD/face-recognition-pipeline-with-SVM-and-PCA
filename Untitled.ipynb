{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313ed83-5c40-4b23-b272-7cab01bf8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_images(url, output_folder, max_images=100):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Fetch the HTML content of the webpage\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find and download images\n",
    "    image_tags = soup.find_all('img', limit=max_images)\n",
    "    for i, img_tag in enumerate(image_tags):\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            # Download the image\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_filename = os.path.join(output_folder, f'image_{i + 1}.jpg')\n",
    "            with open(img_filename, 'wb') as img_file:\n",
    "                img_file.write(img_data)\n",
    "            print(f'Downloaded: {img_filename}')\n",
    "\n",
    "# Example usage:\n",
    "url_to_scrape = 'https://example.com'\n",
    "output_directory = 'downloaded_images'\n",
    "\n",
    "download_images(url_to_scrape, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3ff5b-0888-42a9-bc0f-ceb5b045a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def download_images_recursive(url, output_folder, max_depth=2, max_images=100):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    visited_urls = set()\n",
    "\n",
    "    def download_images_internal(current_url, depth):\n",
    "        if depth > max_depth or current_url in visited_urls:\n",
    "            return\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        image_tags = soup.find_all('img', limit=max_images)\n",
    "        for i, img_tag in enumerate(image_tags):\n",
    "            img_url = img_tag.get('src')\n",
    "            if img_url:\n",
    "                img_url = urljoin(current_url, img_url)\n",
    "                img_data = requests.get(img_url).content\n",
    "                img_filename = os.path.join(output_folder, f'image_{i + 1}.jpg')\n",
    "                with open(img_filename, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "                print(f'Downloaded: {img_filename}')\n",
    "\n",
    "        # Follow links recursively\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            next_url = urljoin(current_url, link['href'])\n",
    "            download_images_internal(next_url, depth + 1)\n",
    "\n",
    "    download_images_internal(url, 0)\n",
    "\n",
    "# Example usage:\n",
    "url_to_scrape = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "output_directory = 'downloaded_images_recursive'\n",
    "\n",
    "download_images_recursive(url_to_scrape, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47585e4a-4421-42a5-8f02-f5d800d32913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def download_images_with_keywords(url, output_folder, keywords, max_depth=2, max_images=100):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    visited_urls = set()\n",
    "\n",
    "    def download_images_internal(current_url, depth):\n",
    "        if depth > max_depth or current_url in visited_urls:\n",
    "            return\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find images with alt attribute containing any of the keywords\n",
    "        image_tags = soup.find_all('img', {'alt': lambda x: x and any(keyword.lower() in x.lower() for keyword in keywords)}, limit=max_images)\n",
    "        for i, img_tag in enumerate(image_tags):\n",
    "            img_url = img_tag.get('src')\n",
    "            if img_url:\n",
    "                img_url = urljoin(current_url, img_url)\n",
    "                img_data = requests.get(img_url).content\n",
    "                # Construct a unique filename based on search keywords and index\n",
    "                img_filename = os.path.join(output_folder, f'{\"_\".join(keywords)}_{i + 1}.jpg')\n",
    "                with open(img_filename, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "                print(f'Downloaded: {img_filename}')\n",
    "\n",
    "        # Follow links recursively\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            next_url = urljoin(current_url, link['href'])\n",
    "            download_images_internal(next_url, depth + 1)\n",
    "\n",
    "    download_images_internal(url, 0)\n",
    "\n",
    "# Example usage:\n",
    "url_to_scrape = 'https://en.wikipedia.org/wiki/Elon_Musk'\n",
    "output_directory = 'downloaded_elon_musk_images'\n",
    "search_keywords = ['elon', 'musk']\n",
    "\n",
    "download_images_with_keywords(url_to_scrape, output_directory, search_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
